#!/usr/bin/env python
"""
Approximate line-count of an extremely large file.
Stops early if possible, if confidence is high.
Has a #Lines relative error tolerance.  Quits if estimate satisfies it at 95% confidence.
"""

# Views the problem as an average bytes-per-line estimator.
# Note for skewed distribution of line lengths (e.g. edgelists), mean estimation is hard.
# e.g. if it really was powerlaw, infinite variance implies infinite standard error for mean estimator.
# Should we use geometric means? but that seems wrong if goal is #lines estimate.
# Is our approach optimal for L2 loss on #line estimate? What about for L1 loss (seems more sensible)?

# Another issue is to think about it way differently as an estimation problem
# of the proportion of \n's in the file.  This may make more sense.

from __future__ import division
import math,sys,os

filename = sys.argv[1]
filesize = os.stat(filename).st_size

scan_limit = int(5e9)  ## how many max bytes to scan, from file?
est_err_thresh =  0.05  ## percent error tolerance, of estimate?  [at 95% confidence]

def print_estimate():
    #print "Num lines: {numlines:.1f} [{numlines_lo:.1f}, {numlines_hi:.1f}], Bytes per Line: {bpl_mean:.1f} (se {bpl_se:.1f})".format(**globals())
    print "Num Lines: {numlines:,.0f} ({err_potential_pct:.1f}% max error), Bytes per Line: {bpl_mean:,.1f} (se {bpl_se:.1f})".format(err_potential_pct=err_potential*100, **globals())

lines_seen = 0
bytes_seen = 0
bpl_ss = 0
for line in open(filename):
    lines_seen += 1
    bytes_seen += len(line)
    bpl_ss += len(line)**2

    # BPL estimator and standard error
    bpl_mean = bytes_seen / lines_seen
    bpl_sd   = math.sqrt((bpl_ss - bytes_seen) / lines_seen)
    bpl_se = bpl_sd / math.sqrt(lines_seen)
    bpl_lo, bpl_hi = bpl_mean-1.96*bpl_se, bpl_mean+1.96*bpl_se
    if bpl_lo < 0: continue

    # Extrapolated interval
    #numlines = filesize / bpl_mean
    #numlines_lo, numlines_hi = filesize/bpl_hi, filesize/bpl_lo

    # Alternative with replacement correction (thanks yucheng)
    n = lines_seen
    b = filesize-bytes_seen
    numlines = n + b/bpl_mean
    numlines_lo, numlines_hi = n + b/bpl_hi,  n + b/bpl_lo

    #if lines_seen % 10000 == 0:
        #print numlines,numlines2, '|', numlines_lo,numlines2_lo,'|',numlines_hi,numlines2_hi
        #print bpl_ss, bytes_seen
        #print "{:,} lines seen, {:,} bytes seen".format(lines_seen, bytes_seen)
        #import json
        #print json.dumps({k:v for k,v in globals().items() if isinstance(v, (int,float))})
        #print_estimate()

    if bytes_seen > scan_limit:
        print "Stopping early"
        break

    # Relative error is calculated as size of interval relative to estimate.
    # Justification...  We assume we're in the 95% case of the true #Lines
    # being inside the interval.  Conditional on that, the worst-case is if the
    # estimate is on the very extreme of the interval, Then you want to bound
    # how far away is the other other very extreme end -- i.e. the size of the
    # interval.

    err_potential = (numlines_hi-numlines_lo) / numlines
    if bpl_lo>0 and err_potential < est_err_thresh:
        print "Confidence satisfied"
        break
else:
    print "Scanned entire file.  Statistics are exact."

#print "{:,} lines seen, {:,} bytes seen".format(lines_seen, bytes_seen)
print_estimate()
